---
类型: 
tags: 
创建时间: 2024-07-18 17:02
完成时间:
---

# 目前深度学习的模型有哪几种，适用于哪些问题？

深度学习是一种基于多层神经网络的机器学习方法，通过构建和训练深度模型来自动学习数据的复杂特征和模式。

深度学习在图像处理、自然语言处理和语音识别等领域取得了显著的成果。
## 目录

1、基础模型架构：

1）卷积神经网络 CNN（Convolutional Neural Network）：

CNN 的实际应用（更适合图像任务）：

2）循环神经网络 RNN (RecurrentNeuralNetwork)：

RNN 的实际应用（更适合NLP任务）：

3）长短期记忆网络 LSTM（long short-term memory）（基于RNN的扩展）

LSTM 的实际应用（更适合NLP任务）：

2、高级模型和技术

1）生成对抗网络 GAN（Generative Adversarial Networks）

GAN 的实际应用：

2）Transformer：

3、大型预训练语言模型（Large Language Model）（通常基于Transformer架构）

思考：大模型时代，学习传统经典模型还有意义吗？

总结

大概从[[基础模型架构]]、高级模型和技术、还有当下最火的 [[LLM大模型]]三个大分类一起来细数一下各分类下主要的模型原理、优缺点和适用的任务。

## 1、**基础模型架构**：

### 1）卷积[神经网络] [CNN]（**Convolutional Neural Network**）：

  

**卷积：** 卷积的本质是通过[[矩阵运算]]的方式将输入数据进行空间上的滤波，有效地提取数据中的局部特征，从而实现特征数据更高程度的抽象表示。

  

![](https://pic1.zhimg.com/80/v2-b21cb9373a6ba49a59094d121c26d99f_1440w.webp?source=2c26e567)

**[池化]：** 可以理解成“压缩”，用来降低[卷积层]输出的特征维度，减少网络参数和计算量。

  

![](https://picx.zhimg.com/80/v2-b19c694c41580a5dfaa1c72054f4149d_1440w.webp?source=2c26e567)

  

CNN通过多个卷积层和[池化层](https://www.zhihu.com/search?q=%E6%B1%A0%E5%8C%96%E5%B1%82&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3332055705%7D)对输入图像进行多次特征提取和降采样，最终得到全局特征表示，再通过[全连接层](https://www.zhihu.com/search?q=%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3332055705%7D)进行分类。

  

**优点：**

参数数量小，训练时间短。

**缺点：**

不适用于[序列数据](https://www.zhihu.com/search?q=%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3332055705%7D)，无法处理时序信息。

  

### **CNN 的实际应用（更适合图像任务）：**

图像分类、[图像检索](https://www.zhihu.com/search?q=%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3332055705%7D)、目标检测、人脸识别、文本分类、语音识别

  

### 2）循环神经网络 RNN ([RecurrentNeuralNetwork)](https://www.zhihu.com/search?q=RecurrentNeuralNetwork%29&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3332055705%7D)：

![](https://picx.zhimg.com/80/v2-5f08f26e310af19dd24b072b3fc41d0c_1440w.webp?source=2c26e567)

RNN是将CNN进行了改造，简单来说就是把上一次的输出混合下一次的输入再做一次计算，每次计算都对上一次的计算结果有一定的依赖。

  

**优点：**

解决了输入数据是连续的序列问题（例如“我喜欢人工智能”每个字之间有时序关系，也有逻辑关系）。

**缺点：**

梯度消失或[梯度爆炸](https://www.zhihu.com/search?q=%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3332055705%7D)、训练速度较慢，计算量较大、复杂序列数据支持得不够好

  

### **RNN 的实际应用（更适合[NLP](https://www.zhihu.com/search?q=NLP&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3332055705%7D)任务）：**

机器翻译、语音识别、图片描述、时间序列分析等任务。

  

### 3）长短期记忆网络 LSTM（[long short-term memory](https://www.zhihu.com/search?q=long%20short-term%20memory&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3332055705%7D)）（基于RNN的扩展）

RNN的工作方式，决定了它不会“遗忘”，前面内容的信息会不断被代入到后续输入中进行计算，从而导致梯度消失或梯度爆炸问题。

  

  

![](https://pica.zhimg.com/80/v2-492d326244f4f8ef8301078d71a24006_1440w.webp?source=2c26e567)

  

LSTM 是RNN的一种变体，通过“门”结构引入“[选择性遗忘](https://www.zhihu.com/search?q=%E9%80%89%E6%8B%A9%E6%80%A7%E9%81%97%E5%BF%98&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3332055705%7D)”机制，解决RNN的这一弊端。

**优点：**

解决了传统RNN中存在的梯度消失和梯度爆炸问题，从而更好地处理长序列数据。

**缺点**

因为 LSTM 是RNN的一种变体，因此缺点方面，仍然是RNN的那些缺点。

  

### **LSTM 的实际应用（更适合NLP任务）：**

文本情感分析、语音识别、机器翻译、序列预测、图像分析、语音生成、[时间序列分析](https://www.zhihu.com/search?q=%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E5%88%86%E6%9E%90&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3332055705%7D)

  

## 2、**高级模型和技术**

### 1）[生成对抗网络](https://www.zhihu.com/search?q=%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3332055705%7D) [GAN](https://www.zhihu.com/search?q=GAN&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3332055705%7D)（Generative Adversarial Networks）

  

GAN由生成器和判别器组成。二者相互对抗，生成器网络负责生成数据并且欺骗判别器网络，而判别器网络负责识别哪些数据是真实的。

  

![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='850' height='443'></svg>)

**优点：**

生成能力强、不需要显式监督

**缺点：**

训练过程复杂、数据要求高

  

### GAN 的实际应用：

图像生成、[图像修复](https://www.zhihu.com/search?q=%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3332055705%7D)、风格迁移、去掉图像遮挡、年龄转移、语音合成等。

  

### 2）Transformer：

2017年，Google发表论文[《Attention is all you need》](https://www.zhihu.com/search?q=%E3%80%8AAttention%20is%20all%20you%20need%E3%80%8B&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3332055705%7D)，成为Transformer开山之作。通过引入自注意力机制、[多头自注意力机制](https://www.zhihu.com/search?q=%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3332055705%7D)、前馈神经网络和位置编码等技术，Transformer实现了高效的并行计算和强大的表示能力。

  

![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='720' height='1044'></svg>)

自此，Transformer架构一路开挂，形成了一个枝繁叶茂的大家族，在文本分类、[命名实体识别](https://www.zhihu.com/search?q=%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3332055705%7D)、[情感分析](https://www.zhihu.com/search?q=%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3332055705%7D)、问答系统、机器翻译、语音识别、图像分类等领域都取得了显著的成果。

  

## 3、**大型预训练语言模型（Large Language Model）（通常基于Transformer架构）**

近年来，BERT、GPT4、[LLaMa](https://www.zhihu.com/search?q=LLaMa&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3332055705%7D)等一大批优秀大模型纷纷亮相，开启了大模型新时代的新篇章。前不久，谷歌多模态大模型Gemini Ultra也重磅发布，如今大模型不能说是热点，而要说是沸点了。


对于技术人员来说，无论是从原理还是从使用上，大模型都注定成为“[兵家必争之地](https://www.zhihu.com/search?q=%E5%85%B5%E5%AE%B6%E5%BF%85%E4%BA%89%E4%B9%8B%E5%9C%B0&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3332055705%7D)”。



## 思考：大模型时代，学习传统经典模型还有意义吗？

  

尽管随着大模型如火如荼的发展，我们不断听到“算法已死”的哀嚎，但是传统经典模型的意义和不容小觑。

  

第一，了解RNN和LSTM等传统模型的基本原理和特点，才能够准确理解和分析业务场景，根据实际情况进行选择和调整，以确保模型能够取得最佳的性能和效果。

  

第二，小模型胜在专而精，小而美，在一些特定场景下，例如车牌识别、人脸识别。。。传统经典模型仍然具有超强的竞争力。

  

第三，传统经典模型提供了更好的可解释性。在如医疗和金融等应用中，模型的可解释性是非常重要的。

  

第四，很多优秀的模型，都是通过融入了其它模型的理念和特质而形成的。通过学习传统经典模型，我们可以更好地利用它们与大型预训练模型的互补性，甚至为之添上神来之笔，通过融合和嫁接做出重大改进。

## 总结

  

在实际应用中，每个分支都有大量的著名的变种模型。

  
  
万变不离其宗，了解了基本原理和设计思想，就能够更容易理解各个变种模型的妙处以及适用场景。


​
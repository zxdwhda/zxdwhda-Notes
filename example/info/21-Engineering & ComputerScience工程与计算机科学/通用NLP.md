---
UID: 20240902201724 
aliases: 
tags: 
source: 
cssclass: 
created: 2024-09-02
---
# 01

## 引言

随着自然语言处理（NLP）的广泛应用以及聊天机器人、机器翻译等成熟商业化产品的推广，各种不同的任务面临着不同的要求，这意味着标准的 NLP 模型无法满足这些不同的要求。另一方面，对于近期的任务，机器需要深入理解意图并流畅地生成自然语言句子。随着深度学习的快速发展，基于深度神经网络的复杂 NLP 模型可以处理更复杂的问题。NLP 的未来似乎大有可为。然而，对于研究人员和从业人员来说，新的困难也在不断涌现。我们面临着更严峻的挑战：寻找高效的 NLP 模型、收集和标注高质量的数据集、提出可行的解决方案以应对有限的计算和存储资源。这些挑战不可能一蹴而就。首先预训练庞大的模型，其次微调或提示模型的两步法是可行的，并已成为新的标准方法。

## 预训练技术
由于标注和收集大型数据集的成本很高，甚至对于某些狭窄领域的任务来说是不可行的，但大量无标注数据显然是可用的，例如维基百科。因此，在自然语言处理领域，在廉价的大规模无标注语料库上预先训练语言模型（PLM）会带来巨大的价值，这意味着 PLM 可以针对下游任务进行快速有效的微调；PLM 可以高效地进行模型初始化，并加速不同任务的收敛；PLM 可以在一定程度上避免目标领域的过拟合。
一般来说，PLM 可分为非上下文嵌入和上下文嵌入。非上下文嵌入法侧重于静态词嵌入，上下文信息非常有限，涉及的模型也相对简单，包括 NNLM 、word2vec （CBOW、Skip-Gram）、Glove。上下文嵌入法通过整合上下文信息来映射词标记，包括 CoVe、ELMo、GPT、BERT、XLNet。

![[99-Attachment/Pasted image 20240902201904.png]]

由于 Transformer 架构在处理 NLP 任务时表现出强大的性能，因此越来越多的最新 PLM 采用了 Transformer 架构。BERT 利用了 Transformer 编码器架构、屏蔽语言建模的优势，是一种双向自动编码语言模型。GPT 利用了 Transformer Decoder 架构，是一种单向自回归语言模型。

![[99-Attachment/Pasted image 20240902201954.png]]

预训练任务可分为经典语言建模、遮蔽语言建模、置换语言建模、去噪自动编码器、对比学习等。通过改进模型结构或整合更多的先验信息（如知识图谱），出现了各种增强型 BERT 或 GPT 扩展模型：例如：RoBerta 、ELECTRA、BART 、T5以及XLNet等。

## 微调技术
与以往的预训练嵌入特征提取技术相比，微调预训练语言模型（PLM）在自然语言处理的各种下游任务的迁移学习中显示出更有效、更强大的作用。一般采用的微调程序包括：第一步，在大量易于收集的无标记数据基础上预训练模型，这种方法被广泛应用于掩码语言建模（MLM）；第二步，在下游任务或领域的特定标记数据上微调 PLM，这种方法采用标准损失函数，如交叉熵损失函数。

![[99-Attachment/Pasted image 20240902202053.png]]

由于 PLM 相对固定，需要更多的计算资源、更庞大的数据集、更高效的模型来更新，但微调可以提供快速的自适应模型，这对许多实际应用来说意义重大。因此，微调是非常有价值的，一些最新的先进方法可以进一步提高微调性能，包括自适应微调、行为微调、参数高效微调、文本到文本微调、缓解微调不稳定性等。

| Category                             | Methods                                                                 | Motivation                                           |
| ------------------------------------ | ----------------------------------------------------------------------- | ---------------------------------------------------- |
| Adaptive Fine-tuning                 | Domain/task/language adaptive fine-tuning                               | Specialise to target domain                          |
| Behavioural Fine-tuning              | Intermediate-task training, self-supervised, frame as MLM               | Specialise to target task                            |
| Parameter-efficient Fine-tuning      | Adapters, sparse parameter permutations, pruning                        | Reduce space of fine-tuned models                    |
| Text-to-text Fine-tuning             | Frame as text-to-text, prompt engineering, controllable NLG             | Effectively use large autoregressive pre-trained LMs |
| Mitigating Fine-tuning Instabilities | Stop runs early, use a small lr, regularisation, avoid random init runs | Reduce variance of fine-tuning runs                  |

**类别** | **方法** | **动机**
---|---|---
自适应微调 | 领域/任务/语言自适应微调 | 专门针对目标领域
行为微调 | 中间任务训练，自我监督，作为多任务学习（MLM）框架 | 专门针对目标任务
参数高效微调 | 适配器，稀疏参数排列，剪枝 | 减少微调模型的空间
文本到文本微调 | 作为文本到文本框架，提示工程，可控自然语言生成（NLG） | 有效地利用大型自回归预训练语言模型
缓解微调不稳定性 | 提前停止运行，使用小的学习率，正则化，避免随机初始化运行 | 减少微调运行的方差

**解读**

这个表格主要讨论了用于改进和优化深度学习模型微调过程的不同策略和技术。微调是一种常用的机器学习技巧，它允许我们用一个已经过大量数据训练过的模型来解决特定的任务或处理特定的数据集。

1. 自适应微调：这种方法专注于调整模型以适应特定的领域、任务或语言。例如，在医疗领域的文本分类任务中，我们可以微调一个已经在大规模语料库上训练过的模型，使其更适合处理医学词汇和概念。
   
2. 行为微调：这种技术通常涉及中间任务训练，即首先让模型完成一些简单的任务，然后逐渐增加难度，最终达到目标任务的要求。此外，自我监督和多任务学习也是提高模型性能的有效手段。

3. 参数高效微调：这一类方法旨在减少微调过程中所需的计算资源。这可以通过引入适配器层、稀疏参数排列或者剪枝等技术来实现。这样做的好处是可以降低微调模型所需的时间和空间复杂度。

4. 文本到文本微调：这是一种将输入转换为输出的方法，如将一段文本转化为另一段文本。在这个过程中，可以采用提示工程和可控自然语言生成（NLG）等技术，从而更有效地利用大型自回归预训练语言模型。

5. 缓解微调不稳定性：最后，为了避免微调过程中出现不稳定的情况，比如模型性能波动大、收敛速度慢等问题，可以采取提前结束训练、减小学习率、正则化以及避免随机初始化等措施。这些方法有助于降低微调运行之间的差异性，使模型更加稳定可靠。

自适应微调技术是将 PLM 转移到更接近目标数据分布的数据分布上。PLM 使用 MLM 方法对相关的未标记数据进行微调，然后使用交叉熵损失对特定任务的标记数据进行微调。转移到目标数据域非常有用，特别是对于专门的不同目标数据分布，这也不可避免地限制了最终模型的泛化能力。

![[99-Attachment/Pasted image 20240902202758.png]]

行为微调更注重任务，首先使用与任务相关的损失函数对任务标注数据进行 PLM 微调，然后再对任务进行微调。这种中间微调训练有利于高级推理和推理，适用于命名实体、解析、语法、答案句子选择、回答问题等任务。
![[99-Attachment/Pasted image 20240902202830.png]]

参数高效微调是指在固定大部分 PLM 的同时，对相当少的参数进行微调，以大大减少下游任务的计算负荷。我们采用适配器，在冻结的 PLM 中插入几个小层。或者，我们也可以微调 PLM 的子集参数，同时冻结剩余的大量参数。

文本到文本的微调是将掩码语言模型转换为自回归模型，方法是用特定任务的头部学习层取代多层语言模型的输出层。  

小目标数据集和随机权重初始化会导致微调模型严重不稳定。因此，将 PLM 模型中间转移到目标领域或任务将缓解不稳定性，对抗或基于信任区域的数据增强方法也很有用。

## 提示工程

随着越来越复杂的 NLP 模型的发展，出现了明显的两难问题：一方面，我们拥有可通用性更强、功能更强大、参数数以亿计的繁琐 PLM，另一方面，下游任务只需要独特、轻量级、快速的自适应模型；一方面，大量未注释的原始数据存在大量偏差和噪声，另一方面，特定任务的注释数据相当少且非常昂贵；一方面，高性能计算中心可以通过大量海量数据训练相当庞大的模型，另一方面，下游任务只能提供有限的计算资源，甚至是智能手机等边缘计算资源；一方面，经典 PLM 的更新时间较长，另一方面，下游任务的需求变化非常快。

总之，二分法导致庞大的通用 PLM 和灵活的轻量级特定任务微调模型，实现成本越来越高。业内总结了第四种应对范式，即预训练、提示、预测。回过头来看，第一种范式没有神经网络，需要大量人工来完成特征工程。第二种范式利用了神经网络，但需要人工进行架构工程，即设计神经网络。第三种范式采用预训练和微调方法，需要人工进行目标工程，为预训练和微调找到合适的损失函数。第四种范式采用预训练、提示、预测方法，需要进行提示挖掘工程以找出适当的提示。

![[99-Attachment/Pasted image 20240902204110.png]]

提示工作流程可分为 4 个步骤：第一个步骤是提示构建，也称为模板，可将所需任务映射到模板中，填充后的模板将输入 PLM 以生成答案，包括掐头去尾提示（模板文本字符串中间未填充的空格）和前缀提示（模板文本字符串开头未填充的空格）；第二种是答案构建，也被称为 "口头化器"，可以将生成的答案制作成任务所需的标签；第三种是答案预测，在实践中，唯一填充的模板将被输入 PLM 以生成相应的答案；第四种是答案标签映射，将相应的答案映射到所需的标签。换句话说，将原始输入 x 填充到模板中，得到提示 x'，该提示有一些空白槽需要填充，然后将提示 x' 输入 PLM，输出完全填充的字符串 x"，该字符串可映射到最终所需的标签 y。

![[99-Attachment/Pasted image 20240902204134.png]]

因此，必须获得一个模板，即提示工程，它可分为手动模板工程、自动模板学习，包括离散提示（进一步包括提示挖掘、提示解析、梯度基搜索、提示生成、提示评分）和连续提示（进一步包括前缀调整、离散提示初始化调整、硬软提示混合调整）。此外，多重提示还能进一步提高提示性能并扩展结果，包括提示组合、提示增强、提示组成和提示分解。虽然生成提示的方法多种多样，但很难进行直接比较以确定最佳方法。此外，有些实验显示，稍加干扰或更换模板，相应的结果就会大相径庭。

## 总结

考虑到小型注释任务特定数据集的局限性、边缘节点计算资源和存储的限制、实时响应的要求、下游任务多样化的快速变化要求，采用预训练模型和事后适应工程成为黄金标准。Transformer模型在 NLP 中被证明是有效的。因此，BERT、GPT 以及后来的增强模型得到了广泛应用。微调任务的重点是根据具体任务调整预训练模型。

因此，为了获得更高的性能，最好采用中间步骤将模型转移到任务数据或任务域中。插入的堆叠适配器可以处理更多的动态任务要求。另外，我们也可以通过提示工程直接从预训练模型中挖掘所需的信息。提示方法对于人类从业者来说非常直接易懂，而且性能强大，但也有学者认为，如果不对下游任务进行微调，就无法实现最佳性能。

